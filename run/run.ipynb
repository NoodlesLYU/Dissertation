{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geojson import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OXF_MSOA_df =  pd.read_excel('data/msoa_data.xlsx', usecols=[\"msoa11cd\"]) # import file with OXF MSOA codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OXF_MSOA_df.columns = ['areakey'] # rename column \"msoa11cd\" to \"areakey\"\n",
    "OXF_MSOA_list = OXF_MSOA_df['areakey'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del OXF_MSOA_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonecodes_EWS = pd.read_csv('data/EWS_ZoneCodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonecodes_EWS.set_index('areakey')\n",
    "zonecodes_EWS_list = zonecodes_EWS['areakey'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import struct\n",
    "import csv\n",
    "\n",
    "def loadQUANTMatrix(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        (m,) = struct.unpack('i', f.read(4))\n",
    "        (n,) = struct.unpack('i', f.read(4))\n",
    "        print(\"loadQUANTMatrix::m=\",m,\"n=\",n)\n",
    "        matrix = np.arange(m*n,dtype=np.float64).reshape(m, n) #and hopefully m===n, but I'm not relying on it\n",
    "        for i in range(0,m):\n",
    "            data = struct.unpack('{0}f'.format(n), f.read(4*n)) #read a row back from the stream - data=list of floats\n",
    "            for j in range(0,n):\n",
    "                matrix[i,j] = data[j]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "cij_road_EWS = loadQUANTMatrix('data/dis_roads_min.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cij_road_EWS_df = pd.DataFrame(cij_road_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cij_road_OXF_df = cij_road_EWS_df[OXF_MSOA_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cij_road_OXF_df = cij_road_OXF_df.loc[OXF_MSOA_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cij_road_OXF = cij_road_OXF_df.to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cij_road_OXF[cij_road_OXF < 1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMatrix(matrix,filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveMatrix(cij_road_OXF, os.path.join('data','Cij_road_min_OXF.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join('data', \"cij_road_OXF.csv\"), cij_road_OXF, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "cij_bus_EWS = loadQUANTMatrix('data/dis_bus_min.bin')\n",
    "cij_bus_EWS_df = pd.DataFrame(cij_bus_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list) # turn the numpy array into a pd dataframe, (index and columns: MSOA codes)\n",
    "cij_bus_OXF_df = cij_bus_EWS_df[OXF_MSOA_list]  # Create OXF df filtering EWS columns\n",
    "cij_bus_OXF_df = cij_bus_OXF_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "cij_bus_OXF = cij_bus_OXF_df.to_numpy()  # numpy matrix for OXF (same format as utils loadQUANTMatrix)\n",
    "cij_bus_OXF[cij_bus_OXF < 1] = 1  # lower limit of 1 minute links\n",
    "saveMatrix(cij_bus_OXF, os.path.join('data','Cij_bus_min_OXF.bin'))\n",
    "        # save as csv file\n",
    "np.savetxt(os.path.join('data', \"cij_bus_OXF.csv\"), cij_bus_OXF, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "cij_rail_EWS = loadQUANTMatrix('data/dis_gbrail_min.bin')\n",
    "cij_rail_EWS_df = pd.DataFrame(cij_rail_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list) # turn the numpy array into a pd dataframe, (index and columns: MSOA codes)\n",
    "cij_rail_OXF_df = cij_rail_EWS_df[OXF_MSOA_list]  # Create OXF df filtering EWS columns\n",
    "cij_rail_OXF_df = cij_rail_OXF_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "cij_rail_OXF = cij_rail_OXF_df.to_numpy()  # numpy matrix for OXF (same format as utils loadQUANTMatrix)\n",
    "cij_rail_OXF[cij_rail_OXF < 1] = 1  # lower limit of 1 minute links\n",
    "saveMatrix(cij_rail_OXF, os.path.join('data', 'Cij_gbrail_min_OXF.bin'))\n",
    "        # save as csv file\n",
    "np.savetxt(os.path.join('data', \"cij_rail_OXF.csv\"), cij_rail_OXF, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "SObs_road_EWS = loadQUANTMatrix('data/SObs_1.bin')\n",
    "SObs_road_EWS_df = pd.DataFrame(SObs_road_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list) # turn the numpy array into a pd dataframe, (index and columns: MSOA codes)\n",
    "SObs_road_OXF_df = SObs_road_EWS_df[OXF_MSOA_list]  # Create OXF df filtering EWS columns\n",
    "SObs_road_OXF_df = SObs_road_OXF_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "SObs_road_OXF = SObs_road_OXF_df.to_numpy()  # numpy matrix for OXF (same format as utils loadQUANTMatrix)\n",
    "saveMatrix(SObs_road_OXF, os.path.join('data', 'SObs_1_OXF.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "SObs_bus_EWS = loadQUANTMatrix('data/SObs_2.bin')\n",
    "SObs_bus_EWS_df = pd.DataFrame(SObs_bus_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list)  # turn the numpy array into a pd dataframe, (index and columns: MSOA codes)\n",
    "SObs_bus_OXF_df = SObs_bus_EWS_df[OXF_MSOA_list]  # Create OXF df filtering EWS columns\n",
    "SObs_bus_OXF_df = SObs_bus_OXF_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "SObs_bus_OXF = SObs_bus_OXF_df.to_numpy()  # numpy matrix for OXF (same format as utils loadQUANTMatrix)\n",
    "saveMatrix(SObs_bus_OXF, os.path.join('data','SObs_2_OXF.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadQUANTMatrix::m= 8436 n= 8436\n"
     ]
    }
   ],
   "source": [
    "SObs_rail_EWS = loadQUANTMatrix('data/SObs_3.bin')\n",
    "SObs_rail_EWS_df = pd.DataFrame(SObs_rail_EWS, index=zonecodes_EWS_list, columns=zonecodes_EWS_list)  # turn the numpy array into a pd dataframe, (index and columns: MSOA codes)\n",
    "SObs_rail_OXF_df = SObs_rail_EWS_df[OXF_MSOA_list]  # Create OXF df filtering EWS columns\n",
    "SObs_rail_OXF_df = SObs_rail_OXF_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "SObs_rail_OXF = SObs_rail_OXF_df.to_numpy()  # numpy matrix for OXF (same format as utils loadQUANTMatrix)\n",
    "saveMatrix(SObs_rail_OXF, os.path.join('data','SObs_3_OXF.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEW = pd.read_csv('data/QS103EW_MSOA.csv')\n",
    "dfEW['count_allpeople'] = dfEW['Age: All categories: Age; measures: Value'] # you could just rename the col, not copy\n",
    "dfEW2 = pd.DataFrame({'geography code': dfEW['geography code'], 'count_allpeople': dfEW['count_allpeople']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfS = pd.read_csv('data/QS103SC_DZ2001.csv') # join on \"Unnamed: 0\", it's blank! This is the datazone code field\n",
    "dfS.set_index('Unnamed: 0')\n",
    "dfSLookup = pd.read_csv('data/DZ2001Lookup.csv') # join on ZONECODE, which is the datazone code\n",
    "dfS = dfS.join(other=dfSLookup.set_index('ZONECODE'),on='Unnamed: 0')\n",
    "dfS['count_allpeople'] = dfS['All people']\n",
    "dfS2 = dfS.groupby(['IZ_CODE']).agg({'count_allpeople': \"sum\"})\n",
    "dfS3 = pd.DataFrame({'msoaiz': dfS2.index, 'count_allpeople': dfS2['count_allpeople'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEW2.reset_index()\n",
    "dfS3.reset_index()\n",
    "dfEW2.columns=['msoaiz','count_allpeople']\n",
    "dfEWS = pd.concat([dfEW2,dfS3])\n",
    "dfEWS.to_csv('data/totalpopulation_englandwalesscotland_msoaiz.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPopMSOAPopulation_EWS = pd.read_csv('data/totalpopulation_englandwalesscotland_msoaiz.csv', usecols=['msoaiz', 'count_allpeople'], index_col='msoaiz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPopMSOAPopulation_OXF = dfPopMSOAPopulation_EWS.loc[OXF_MSOA_list]  # Filter rows\n",
    "dfPopMSOAPopulation_OXF.sort_index(inplace=True)\n",
    "dfPopMSOAPopulation_OXF['msoaiz'] = dfPopMSOAPopulation_OXF.index  # turn the index (i.e. MSOA codes) back into a columm\n",
    "dfPopMSOAPopulation_OXF.reset_index(drop=True, inplace=True)  # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "popretailPopulation = dfPopMSOAPopulation_OXF.join(other=zonecodes_EWS.set_index('areakey'), on='msoaiz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "popretailPopulation.to_csv('data/populationretailPopulation_OXF_2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailpoints_EWS = pd.read_csv('data/geolytix_retailpoints_open_regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OXF_postcodes_df = pd.read_excel('data/Oxfordshire_postcodes.xlsx')  # df containing: Postcode,Latitude,Longitude,Eastings,Northings\n",
    "OXF_postcodes_df['DistrictCode'] = OXF_postcodes_df['pcd'].str[:-3]  # create column with district code\n",
    "OXF_DistrictCodes = OXF_postcodes_df['DistrictCode'].tolist()  # save the district codes into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_geolitix_OXF = retailpoints_EWS.loc[retailpoints_EWS['postcode'].str[:-4].isin(OXF_DistrictCodes)]  # Filter rows (-4 because of the blank space in the middle of the postcode)\n",
    "open_geolitix_OXF.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_geolitix_OXF.to_csv('data/geolytix_retailpoints_open_regression_OXF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QUANTLHModel:\n",
    "    \"\"\"\n",
    "    constructor\n",
    "    @param n number of residential zones (MSOA)\n",
    "    @param m number of school point zones\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, n):\n",
    "        # constructor\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.Ei = np.zeros(m)\n",
    "        self.Aj = np.zeros(n)\n",
    "        self.cij_0 = np.zeros(1)  # costs matrix - set to something BIG later\n",
    "        self.cij_1 = np.zeros(1)  # costs matrix - set to something BIG later\n",
    "        self.cij_2 = np.zeros(1)  # costs matrix - set to something BIG later\n",
    "        self.SObs_0 = np.zeros(1)\n",
    "        self.SObs_1 = np.zeros(1)\n",
    "        self.SObs_2 = np.zeros(1)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    setPopulationVectorEi\n",
    "    Overload of setPopulationEi to set Ei directly from a vector, rather than a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    def setPopulationVectorEi(self, Ei):\n",
    "        self.Ei = Ei\n",
    "        assert len(self.Ei) == self.m, \"FATAL: setPopulationEi length Ei=\" + str(\n",
    "            len(self.Ei)) + \" MUST equal model definition size of m=\" + str(self.m)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    setPopulationEi\n",
    "    Given a data frame containing one column with the zone number (i) and one column\n",
    "    with the actual data values, fill the Ei population property with data so that\n",
    "    the position in the Ei numpy array is the zonei field of the data.\n",
    "    The code is almost identical to the setAttractorsAj method.\n",
    "    NOTE: max(i) < self.m\n",
    "    \"\"\"\n",
    "    def setPopulationEi(self, df, zoneiColName, dataColName):\n",
    "        df2 = df.sort_values(by=[zoneiColName])\n",
    "        self.Ei = df2[dataColName].to_numpy()\n",
    "        assert len(self.Ei) == self.m, \"FATAL: setPopulationEi length Ei=\" + str(len(self.Ei)) + \" MUST equal model definition size of m=\" + str(self.m)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    setAttractorsAj\n",
    "    Given a data frame containing one column with the zone number (j) and one column\n",
    "    with the actual data values, fill the Aj attractors property with data so that\n",
    "    the position in the Aj numpy array is the zonej field of the data.\n",
    "    The code is almost identical to the setPopulationEi method.\n",
    "    NOTE: max(j) < self.n\n",
    "    \"\"\"\n",
    "    def setAttractorsAj(self, df, zonejColName, dataColName):\n",
    "        df2 = df.sort_values(by=[zonejColName])\n",
    "        self.Aj = df2[dataColName].to_numpy()\n",
    "        assert len(self.Aj) == self.n, \"FATAL: setAttractorsAj length Aj=\" + str(len(self.Aj)) + \" MUST equal model definition size of n=\" + str(self.n)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    setCostsMatrix\n",
    "    Assign the cost matrix for the model to use when it runs.\n",
    "    NOTE: this MUST match the m x n order of the model and be a numpy array\n",
    "    \"\"\"\n",
    "    def setCostMatrixCij(self, cij_0, cij_1, cij_2):\n",
    "        i, j = cij_0.shape  # Hopefully cij_0.shape = cij_1.shape = cij_2.shape\n",
    "        assert i == self.m, \"FATAL: setCostsMatrix cij matrix is the wrong size, cij.m=\" + str(i) + \" MUST match model definition of m=\" + str(self.m)\n",
    "        assert j == self.n, \"FATAL: setCostsMatrix cij matrix is the wrong size, cij.n=\" + str(j) + \" MUST match model definition of n=\" + str(self.n)\n",
    "        self.cij_0 = cij_0\n",
    "        self.cij_1 = cij_1\n",
    "        self.cij_2 = cij_2\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    setObsMatrix\n",
    "    Assign the cost matrix for the model to use when it runs.\n",
    "    NOTE: this MUST match the m x n order of the model and be a numpy array\n",
    "    \"\"\"\n",
    "    def setObsMatrix(self, SObs_0, SObs_1, SObs_2):\n",
    "        i, j = SObs_0.shape  # Hopefully cij_0.shape = SObs_0.shape = SObs_0.shape\n",
    "        assert i == self.m, \"FATAL: setCostsMatrix cij matrix is the wrong size, cij.m=\" + str(\n",
    "            i) + \" MUST match model definition of m=\" + str(self.m)\n",
    "        assert j == self.n, \"FATAL: setCostsMatrix cij matrix is the wrong size, cij.n=\" + str(\n",
    "            j) + \" MUST match model definition of n=\" + str(self.n)\n",
    "        self.SObs_0 = SObs_0\n",
    "        self.SObs_1 = SObs_1\n",
    "        self.SObs_2 = SObs_2\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    computeCBar\n",
    "    Compute average trip length TODO: VERY COMPUTATIONALLY INTENSIVE - FIX IT\n",
    "    @param Sij trips matrix containing the flow numbers between MSOA (i) and schools (j)\n",
    "    @param cij trip times between i and j\n",
    "    \"\"\"\n",
    "    def computeCBar(self, Sij, cij):\n",
    "        CNumerator = np.sum(Sij * cij)\n",
    "        CDenominator = np.sum(Sij)\n",
    "        cbar = CNumerator / CDenominator\n",
    "        return cbar\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "        Calculate Dj for a trips matrix.\n",
    "        Two methods are presented here, one which is simple and very slow and one\n",
    "        which uses python vector maths and is much faster. Once 2 is proven equal\n",
    "        to 1, then it can be used exclusively. This function is mainly used for\n",
    "        testing with the TensorFlow and other implementations.\n",
    "        \"\"\"\n",
    "\n",
    "    def calculateDj(self, Tij):\n",
    "        (M, N) = np.shape(Tij)\n",
    "        Dj = np.zeros(N)\n",
    "        Dj = Tij.sum(axis=0)\n",
    "        return Dj\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    run Model run3modes\n",
    "    Quant model for three modes of transport\n",
    "    @returns Sij predicted flows between i and j\n",
    "    \"\"\"\n",
    "    def run3modes(self):\n",
    "        # run model\n",
    "        # i = employment zone\n",
    "        # j = residential zone\n",
    "        # Ei = number of jobs at MSOA location\n",
    "        # Aj = attractor of HH (number of dwellings)\n",
    "        # cij_mode = travel cost for \"mode\" (i.e. road, bus, rail)\n",
    "        # Modes: Road = 0, Bus = 1, Rail = 2\n",
    "        # Beta = Beta values for three modes - this is also output\n",
    "        # QUANT data:\n",
    "        # Observed trips data: \"SObs_1.bin\", \"SObs_2.bin\", \"SObs_3.bin\"\n",
    "        # Travel cost per mode: \"dis_roads_min.bin\", \"dis_bus_min.bin\", \"dis_gbrail_min.bin\"\n",
    "        # Note the use of 1,2,3 for modes in the files different from 0,1,2 in the code.\n",
    "        # Returns predicted flows per mode: \"SPred_1.bin\", \"SPred_2.bin\", \"SPred_3.bin\"\n",
    "\n",
    "        # Initialisation of parameters\n",
    "        converged = False  # initialise convergence check param\n",
    "        n_modes = 3  # Number of modes\n",
    "        cij_k = [self.cij_0, self.cij_1, self.cij_2]  # list of cost matrices\n",
    "        SObs_k = [self.SObs_0, self.SObs_1, self.SObs_2]  # list of obs trips matrices\n",
    "\n",
    "        # Set up Beta for modes 0, 1 and 2 to 1.0\n",
    "        Beta = np.ones(n_modes)\n",
    "\n",
    "        # Compute sum of origins and destinations\n",
    "\n",
    "        '''\n",
    "        # OiObs : vector with dimension = number of oringins\n",
    "        OiObs = np.zeros(self.n)\n",
    "        for k in range(n_modes):\n",
    "            OiObs += SObs_k[k].sum(axis=1)\n",
    "        '''\n",
    "\n",
    "        # DjObs : vector with dimension = number of destinations\n",
    "        DjObs = [[] for i in range(n_modes)]\n",
    "        for k in range(n_modes):\n",
    "            DjObs[k] = np.zeros(self.n)\n",
    "        for k in range(n_modes):\n",
    "            DjObs[k] += SObs_k[k].sum(axis=1)\n",
    "\n",
    "        DjPred = [[] for i in range(n_modes)]\n",
    "        DjPredSum = np.zeros(n_modes)\n",
    "        DjObsSum = np.zeros(n_modes)\n",
    "        delta = np.zeros(n_modes)\n",
    "\n",
    "\n",
    "        # Convergence loop:\n",
    "        print(\"Calibrating the model...\")\n",
    "        iteration = 0\n",
    "        while converged != True:\n",
    "            iteration += 1\n",
    "            print(\"Iteration: \", iteration)\n",
    "\n",
    "            # Initialise variables:\n",
    "            Sij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty lists equal to n_modes\n",
    "\n",
    "            # hold copy of pre multiplied copies of -Beta_k * cij[k] for each mode\n",
    "            ExpMBetaCijk = [[] for k in range(n_modes)]\n",
    "            for kk in range(n_modes):\n",
    "                ExpMBetaCijk[kk] = np.exp(-Beta[kk] * cij_k[kk])\n",
    "\n",
    "            for k in range(n_modes):  # mode loop\n",
    "                Sij[k] = np.zeros(self.m * self.n).reshape(self.m, self.n)\n",
    "                for i in range(self.m):\n",
    "                    denom = 0\n",
    "                    for kk in range(n_modes):\n",
    "                        denom += np.sum(self.Aj * ExpMBetaCijk[kk][i, :])\n",
    "                    Sij2 = self.Ei[i] * (self.Aj * ExpMBetaCijk[k][i, :] / denom)\n",
    "                    Sij[k][i, :] = Sij2  # put answer slice back in return array\n",
    "\n",
    "            # Calibration with CBar values\n",
    "            # Calculate mean predicted trips and mean observed trips (this is CBar)\n",
    "            CBarPred = np.zeros(n_modes)\n",
    "            CBarObs = np.zeros(n_modes)\n",
    "            delta = np.ones(n_modes)\n",
    "            for k in range(n_modes):\n",
    "                CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "                CBarObs[k] = self.computeCBar(SObs_k[k], cij_k[k])\n",
    "                delta[k] = np.absolute(CBarPred[k] - CBarObs[k])  # the aim is to minimise delta[0]+delta[1]+...\n",
    "            # delta check on all betas (Beta0, Beta1, Beta2) stopping condition for convergence\n",
    "            # double gradient descent search on Beta0 and Beta1 and Beta2\n",
    "            converged = True\n",
    "            for k in range(n_modes):\n",
    "                if (delta[k] / CBarObs[k] > 0.001):\n",
    "                    Beta[k] = Beta[k] * CBarPred[k] / CBarObs[k]\n",
    "                    converged = False\n",
    "            '''\n",
    "            # Calibration with Observed flows\n",
    "            for k in range(n_modes):\n",
    "                DjPred[k] = self.calculateDj(Sij[k])\n",
    "                DjPredSum[k] = np.sum(DjPred[k])\n",
    "                DjObsSum[k] = np.sum(DjObs[k])\n",
    "                delta[k] = DjPredSum[k] - DjObsSum[k]\n",
    "            # delta check on beta stopping condition for convergence\n",
    "            # gradient descent search on beta\n",
    "            converged = True\n",
    "            for k in range(n_modes):\n",
    "                if (delta[k] / DjObsSum[k] > 0.001):\n",
    "                    Beta[k] = Beta[k] * DjPredSum[k] / DjObsSum[k]\n",
    "                    converged = False\n",
    "            '''\n",
    "            CBarPred = np.zeros(n_modes)\n",
    "            # Calculate CBar\n",
    "            for k in range(n_modes):\n",
    "                CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "\n",
    "            # Debug:\n",
    "            # commuter sum blocks\n",
    "            TotalSij_roads = Sij[0].sum()\n",
    "            TotalSij_bus = Sij[1].sum()\n",
    "            TotalSij_rail = Sij[2].sum()\n",
    "            TotalEi = self.Ei.sum()  # total jobs = pu+pr above\n",
    "            # print(\"i= {0:d} beta_roads={1:.6f} beta_bus={2:.6f} beta_rail={3:.6f} cbar_pred_roads={4:.1f} cbar_pred_busr={5:.1f} cbar_pred_rail={6:.1f}\"\n",
    "            #         .format(iteration, Beta[0], Beta[1], Beta[2], CBarPred[0], CBarPred[1], CBarPred[2]))\n",
    "            # print(\"TotalSij_roads={0:.1f} TotalSij_bus={1:.1f} TotalSij_rail={2:.1f} Total={3:.1f} ({4:.1f})\"\n",
    "            #       .format(TotalSij_roads, TotalSij_bus, TotalSij_rail, TotalSij_roads + TotalSij_bus + TotalSij_rail, TotalEi))\n",
    "\n",
    "        return Sij, Beta, CBarPred  # Note that Sij = [Sij_k=0 , Sij_k=1, Sij_k=2] and CBarPred = [CBarPred_0, CBarPred_1, CBarPred_2]\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    run Model run3modes_NoCalibration\n",
    "    Quant model for three modes of transport without calibration\n",
    "    @returns Sij predicted flows between i and j\n",
    "    \"\"\"\n",
    "    def run3modes_NoCalibration(self, Beta):\n",
    "        n_modes = len(Beta)  # Number of modes\n",
    "        print(\"Running model for \", n_modes, \" modes.\")\n",
    "\n",
    "        # Initialise variables:\n",
    "        Sij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty lists equal to n_modes\n",
    "        cij_k = [self.cij_0, self.cij_1, self.cij_2]  # list of cost matrices\n",
    "\n",
    "        # hold copy of pre multiplied copies of -Beta_k * cij[k] for each mode\n",
    "        ExpMBetaCijk = [[] for k in range(n_modes)]\n",
    "        for kk in range(n_modes):\n",
    "            ExpMBetaCijk[kk] = np.exp(-Beta[kk] * cij_k[kk])\n",
    "\n",
    "        for k in range(n_modes):  # mode loop\n",
    "            Sij[k] = np.zeros(self.m * self.n).reshape(self.m, self.n)\n",
    "            for i in range(self.m):\n",
    "                denom = 0\n",
    "                for kk in range(n_modes):\n",
    "                    denom += np.sum(self.Aj * ExpMBetaCijk[kk][i, :])\n",
    "                Sij2 = self.Ei[i] * (self.Aj * ExpMBetaCijk[k][i, :] / denom)\n",
    "                Sij[k][i, :] = Sij2  # put answer slice back in return array\n",
    "\n",
    "        CBarPred = np.zeros(n_modes)  # initialise CBarPred\n",
    "        for k in range(n_modes):\n",
    "            CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "\n",
    "        return Sij, CBarPred\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    computeProbabilities3modes\n",
    "    Compute the probability of a flow from an MSOA zone to any (i.e. all) of the possible point zones\n",
    "    \"\"\"\n",
    "    def computeProbabilities3modes(self, Sij):\n",
    "        print(\"Computing probabilities\")\n",
    "        n_modes = 3\n",
    "        probSij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty list equal to n_modes\n",
    "\n",
    "        for k in range(n_modes):\n",
    "            probSij[k] = np.arange(self.m * self.n, dtype=np.float64).reshape(self.m, self.n)\n",
    "            for i in range(self.m):\n",
    "                sum = np.sum(Sij[k][i,])\n",
    "                if sum <= 0:\n",
    "                    sum = 1  # catch for divide by zero - just let the zero probs come through to the final matrix\n",
    "                probSij[k][i,] = Sij[k][i,] / sum\n",
    "\n",
    "        return probSij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QUANTRetailModel(QUANTLHModel):\n",
    "    \"\"\"\n",
    "    constructor\n",
    "    @param n number of residential zones\n",
    "    @param m number of retail zones\n",
    "    \"\"\"\n",
    "    def __init__(self,m,n):\n",
    "        #constructor\n",
    "        super().__init__(m,n)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    loadGeolytixData\n",
    "    @param filename Name of file to load - this is the Geolytix restricted access data with\n",
    "    the floorspace and retail data\n",
    "    @returns DataFrame containing [key,zonei,east,north] and [zonei,Modelled turnover annual]\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def loadGeolytixData(filename):\n",
    "        missing_values = ['-', 'n/a', 'na', '--', ' -   ']\n",
    "        df = pd.read_csv(filename,usecols=['id','fascia','modelled sq ft','Modelled turnover annual','bng_e','bng_n'], na_values=missing_values)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True) # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "        dfzones = pd.DataFrame({'id':df.id,'zonei':df.index,'east':df.bng_e,'north':df.bng_n})\n",
    "        dfattractors = pd.DataFrame({'zonei':df.index,'Modelled turnover annual':df['Modelled turnover annual']}) # could also used floorspace\n",
    "        return dfzones, dfattractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "popretailZones, popretailAttractors = QUANTRetailModel.loadGeolytixData('data/geolytix_retailpoints_open_regression_OXF.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "popretailZones.to_csv('data/populationretailZones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "popretailAttractors.to_csv('data/populationretailAttractors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "def costMSOAToPoint_3modes(cij_roads, cij_bus, cij_rail, dfPoints, OXF_MSOA_list):\n",
    "    # const to define what speed we travel the additional distance to the retail point e.g. 30mph = 13 ms-1\n",
    "    metresPerSec_roads = 13.0 # 30 miles/h = 47 km/h\n",
    "    metresPerSec_bus = 13.0  # 30 miles/h = 47 km/h\n",
    "    metresPerSec_rail = 13.0  # 30 miles/h = 47 km/h\n",
    "\n",
    "    # read in the road, bus and rail centroids for the cij matrix\n",
    "    df_roads = pd.read_csv(os.path.join('data', 'roadcentroidlookup_QC.csv'), index_col='zonecode')\n",
    "    df_bus = pd.read_csv(os.path.join('data', 'buscentroidlookup_QC.csv'), index_col='zonecode')\n",
    "    df_rail = pd.read_csv(os.path.join('data', 'gbrailcentroidlookup_QC.csv'), index_col='zonecode')\n",
    "\n",
    "    # Filter out Oxfordshire from EWS files\n",
    "    df_roads = df_roads.loc[OXF_MSOA_list]  # Filter rows\n",
    "    df_bus = df_bus.loc[OXF_MSOA_list]  # Filter rows\n",
    "    df_rail = df_rail.loc[OXF_MSOA_list]  # Filter rows\n",
    "\n",
    "    df_roads['zonecode'] = df_roads.index  # turn the index (i.e. MSOA codes) back into a columm\n",
    "    df_bus['zonecode'] = df_bus.index  # turn the index (i.e. MSOA codes) back into a columm\n",
    "    df_rail['zonecode'] = df_rail.index  # turn the index (i.e. MSOA codes) back into a columm\n",
    "\n",
    "    # Reset indexes\n",
    "    df_roads.reset_index(drop=True, inplace=True)\n",
    "    df_bus.reset_index(drop=True, inplace=True)\n",
    "    df_rail.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Overwrite the zonei column with the new index\n",
    "    df_roads['zonei'] = df_roads.index\n",
    "    df_bus['zonei'] = df_bus.index\n",
    "    df_rail['zonei'] = df_rail.index\n",
    "\n",
    "    # code this into a geodataframe so we can make a spatial index\n",
    "    gdf_roads = gpd.GeoDataFrame(df_roads, crs='epsg:4326', geometry=gpd.points_from_xy(df_roads.vertex_lon, df_roads.vertex_lat))\n",
    "    gdf_bus = gpd.GeoDataFrame(df_bus, crs='epsg:4326', geometry=gpd.points_from_xy(df_bus.vertex_lon, df_bus.vertex_lat))\n",
    "    gdf_rail = gpd.GeoDataFrame(df_rail, crs='epsg:4326', geometry=gpd.points_from_xy(df_rail.vertex_lon, df_rail.vertex_lat))\n",
    "\n",
    "    # but it's lat/lon and we want east/north, convert crs:\n",
    "    centroids_roads = gdf_roads.to_crs(\"EPSG:27700\")\n",
    "    centroids_bus = gdf_bus.to_crs(\"EPSG:27700\")\n",
    "    centroids_rail = gdf_rail.to_crs(\"EPSG:27700\")\n",
    "\n",
    "    dest_unary_roads = centroids_roads[\"geometry\"].unary_union  # and need this join for the centroid points nearest lookup\n",
    "    dest_unary_bus = centroids_bus[\"geometry\"].unary_union  # and need this join for the centroid points nearest lookup\n",
    "    dest_unary_rail = centroids_rail[\"geometry\"].unary_union  # and need this join for the centroid points nearest lookup\n",
    "\n",
    "    # create a new MSOA to points cost matix\n",
    "    m, n = cij_roads.shape\n",
    "    p, cols = dfPoints.shape\n",
    "    # print(\"array size = \", p, m)\n",
    "\n",
    "    cijpoint_roads = np.zeros(m * p, dtype=np.float64).reshape(m, p)  # so m=MSOA and p=points index\n",
    "    cijpoint_bus = np.zeros(m * p, dtype=np.float64).reshape(m, p)  # so m=MSOA and p=points index\n",
    "    cijpoint_rail = np.zeros(m * p, dtype=np.float64).reshape(m, p)  # so m=MSOA and p=points index\n",
    "\n",
    "    # now make the amended cost function\n",
    "    count = 0\n",
    "    for row in dfPoints.itertuples(index=False):  # NOTE: iterating over Pandas rows is supposed to be bad - how else to do this?\n",
    "        if (count % 50 == 0): print(\"costs::costMSOAToPoint \", count, \"/\", p)\n",
    "        count += 1\n",
    "\n",
    "        p_zonei = getattr(row, 'zonei')\n",
    "        p_east = getattr(row, 'east')\n",
    "        p_north = getattr(row, 'north')\n",
    "\n",
    "        near_roads = nearest_points(Point(p_east, p_north), dest_unary_roads)\n",
    "        near_bus = nearest_points(Point(p_east, p_north), dest_unary_bus)\n",
    "        near_rail = nearest_points(Point(p_east, p_north), dest_unary_rail)\n",
    "\n",
    "        match_geom_roads = centroids_roads.loc[centroids_roads.geometry == near_roads[1]]\n",
    "        match_geom_bus = centroids_bus.loc[centroids_bus.geometry == near_bus[1]]\n",
    "        match_geom_rail = centroids_rail.loc[centroids_rail.geometry == near_rail[1]]\n",
    "\n",
    "        pmsoa_zonei_roads = int(match_geom_roads.zonei.iloc[0])  # closest point msoa zone\n",
    "        pmsoa_zonei_bus = int(match_geom_bus.zonei.iloc[0])  # closest point msoa zone\n",
    "        pmsoa_zonei_rail = int(match_geom_rail.zonei.iloc[0])  # closest point msoa zone\n",
    "\n",
    "        pmsoa_pt_roads = match_geom_roads.geometry\n",
    "        pmsoa_pt_bus = match_geom_bus.geometry\n",
    "        pmsoa_pt_rail = match_geom_rail.geometry\n",
    "\n",
    "        pmsoa_east_roads = float(pmsoa_pt_roads.centroid.x.iloc[0])\n",
    "        pmsoa_east_bus = float(pmsoa_pt_bus.centroid.x.iloc[0])\n",
    "        pmsoa_east_rail = float(pmsoa_pt_rail.centroid.x.iloc[0])\n",
    "\n",
    "        pmsoa_north_roads = float(pmsoa_pt_roads.centroid.y.iloc[0])\n",
    "        pmsoa_north_bus = float(pmsoa_pt_bus.centroid.y.iloc[0])\n",
    "        pmsoa_north_rail = float(pmsoa_pt_rail.centroid.y.iloc[0])\n",
    "\n",
    "        dx_roads = p_east - pmsoa_east_roads\n",
    "        dx_bus = p_east - pmsoa_east_bus\n",
    "        dx_rail = p_east - pmsoa_east_rail\n",
    "\n",
    "        dy_roads = p_north - pmsoa_north_roads\n",
    "        dy_bus = p_north - pmsoa_north_bus\n",
    "        dy_rail = p_north - pmsoa_north_rail\n",
    "\n",
    "        dist_roads = np.sqrt(dx_roads * dx_roads + dy_roads * dy_roads)  # dist between point and centroid used for shortest path\n",
    "        dist_bus = np.sqrt(dx_bus * dx_bus + dy_bus * dy_bus)  # dist between point and centroid used for shortest path\n",
    "        dist_rail = np.sqrt(dx_rail * dx_rail + dy_rail * dy_rail)  # dist between point and centroid used for shortest path\n",
    "\n",
    "        # work out an additional delta cost based on increased time getting from this point to the centroid\n",
    "        deltaCost_roads = (dist_roads / metresPerSec_roads) / 60.0  # transit time in mins\n",
    "        deltaCost_bus = (dist_bus / metresPerSec_bus) / 60.0  # transit time in mins\n",
    "        deltaCost_rail = (dist_rail / metresPerSec_rail) / 60.0  # transit time in mins\n",
    "\n",
    "        # now write every cij value for msoa_zonei to p_zonei (closest) PLUS deltaCose for p_zonei to actual point\n",
    "        for i in range(n):\n",
    "            C1_roads = cij_roads[pmsoa_zonei_roads, i]  # yes, this is right for a trip from MSOA to closest point MSOA - QUANT is BACKWARDS\n",
    "            C1_bus = cij_bus[pmsoa_zonei_bus, i]  # yes, this is right for a trip from MSOA to closest point MSOA - QUANT is BACKWARDS\n",
    "            C1_rail = cij_rail[pmsoa_zonei_rail, i]  # yes, this is right for a trip from MSOA to closest point MSOA - QUANT is BACKWARDS\n",
    "\n",
    "            cijpoint_roads[i, p_zonei] = C1_roads + deltaCost_roads\n",
    "            cijpoint_bus[i, p_zonei] = C1_bus + deltaCost_bus\n",
    "            cijpoint_rail[i, p_zonei] = C1_rail + deltaCost_rail\n",
    "\n",
    "            # NOTE: you can only go in one direction with a matrix that is asymmetric\n",
    "\n",
    "    return cijpoint_roads, cijpoint_bus, cijpoint_rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costs::costMSOAToPoint  0 / 520\n",
      "costs::costMSOAToPoint  50 / 520\n",
      "costs::costMSOAToPoint  100 / 520\n",
      "costs::costMSOAToPoint  150 / 520\n",
      "costs::costMSOAToPoint  200 / 520\n",
      "costs::costMSOAToPoint  250 / 520\n",
      "costs::costMSOAToPoint  300 / 520\n",
      "costs::costMSOAToPoint  350 / 520\n",
      "costs::costMSOAToPoint  400 / 520\n",
      "costs::costMSOAToPoint  450 / 520\n",
      "costs::costMSOAToPoint  500 / 520\n"
     ]
    }
   ],
   "source": [
    "retailpoints_cij_roads, retailpoints_cij_bus, retailpoints_cij_rail = costMSOAToPoint_3modes(cij_road_OXF, cij_bus_OXF, cij_rail_OXF, popretailZones, OXF_MSOA_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveMatrix(retailpoints_cij_roads, 'data/retailpointsCij_roads.bin')\n",
    "saveMatrix(retailpoints_cij_bus, 'data/retailpointsCij_bus.bin')\n",
    "saveMatrix(retailpoints_cij_rail, 'data/retailpointsCij_rail.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/retailpointsCij_roads.csv', retailpoints_cij_roads, delimiter=\",\")\n",
    "np.savetxt('data/retailpointsCij_bus.csv', retailpoints_cij_bus, delimiter=\",\")\n",
    "np.savetxt('data/retailpointsCij_rail.csv', retailpoints_cij_rail, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = retailpoints_cij_roads.shape\n",
    "model = QUANTRetailModel(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setAttractorsAj(popretailAttractors,'zonei','Modelled turnover annual')\n",
    "model.setPopulationEi(popretailPopulation,'zonei','count_allpeople')\n",
    "model.setCostMatrixCij(retailpoints_cij_roads, retailpoints_cij_bus, retailpoints_cij_rail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3modes_NoCalibration(self, Beta):\n",
    "        n_modes = len(Beta)  # Number of modes\n",
    "        print(\"Running model for \", n_modes, \" modes.\")\n",
    "\n",
    "        # Initialise variables:\n",
    "        Sij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty lists equal to n_modes\n",
    "        cij_k = [self.cij_0, self.cij_1, self.cij_2]  # list of cost matrices\n",
    "\n",
    "        # hold copy of pre multiplied copies of -Beta_k * cij[k] for each mode\n",
    "        ExpMBetaCijk = [[] for k in range(n_modes)]\n",
    "        for kk in range(n_modes):\n",
    "            ExpMBetaCijk[kk] = np.exp(-Beta[kk] * cij_k[kk])\n",
    "\n",
    "        for k in range(n_modes):  # mode loop\n",
    "            Sij[k] = np.zeros(self.m * self.n).reshape(self.m, self.n)\n",
    "            for i in range(self.m):\n",
    "                denom = 0\n",
    "                for kk in range(n_modes):\n",
    "                    denom += np.sum(self.Aj * ExpMBetaCijk[kk][i, :])\n",
    "                Sij2 = self.Ei[i] * (self.Aj * ExpMBetaCijk[k][i, :] / denom)\n",
    "                Sij[k][i, :] = Sij2  # put answer slice back in return array\n",
    "\n",
    "        CBarPred = np.zeros(n_modes)  # initialise CBarPred\n",
    "        for k in range(n_modes):\n",
    "            CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "\n",
    "        return Sij, CBarPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jobs data for MSOA residential zones\n",
    "dfEi = pd.read_csv('data/Employment_by_sector_GB_MSOA.csv', usecols=['geography code','Industry: All categories: Industry; measures: Value'], index_col='geography code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEi.rename(columns={'geography code': 'msoa', 'Industry: All categories: Industry; measures: Value': 'employment_tot'}, inplace=True)\n",
    "\n",
    "    # Filter out Oxfordshire from EW dataset:\n",
    "dfEi = dfEi.loc[OXF_MSOA_list]  # Filter rows\n",
    "\n",
    "dfEi['msoa'] = dfEi.index # turn the index (i.e. MSOA codes) back into a columm\n",
    "\n",
    "dfEi.reset_index(drop=True, inplace=True)  # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "\n",
    "    # drop columns:\n",
    "dfEi = dfEi[['msoa','employment_tot']]\n",
    "jobs = dfEi.join(other=zonecodes_EWS.set_index('areakey'), on='msoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_census_QS103_EW_df = pd.read_csv('data/QS103EW_MSOA.csv', index_col='geography code')\n",
    "data_census_QS103_OXF_df = data_census_QS103_EW_df.loc[OXF_MSOA_list]  # Filter rows\n",
    "data_census_QS103_OXF_df['geography code'] = data_census_QS103_OXF_df.index # turn the index (i.e. MSOA codes) back into a columm\n",
    "data_census_QS103_OXF_df.reset_index(drop=True, inplace=True)\n",
    "data_census_QS103_OXF_df.to_csv('data/QS103EW_MSOA_OXF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "utils.py\n",
    "Data building utilities\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import struct\n",
    "import csv\n",
    "\n",
    "###############################################################################\n",
    "\"\"\"\n",
    "Load the zone codes lookup from a csv file into a dictionary of dictionaries\n",
    "\"\"\"\n",
    "#MOVED TO zonecodes.py\n",
    "# def loadZoneLookup(filename):\n",
    "#     ZoneLookup = {}\n",
    "#     with open(filename,'r') as csv_file:\n",
    "#         reader = csv.reader(csv_file, delimiter=',')\n",
    "#         header = next(reader,None) #skip header line\n",
    "#         for row in reader:\n",
    "#             #zonei,areakey,name,east,north\n",
    "#             #0,E02000001,City of London 001,532482.7,181269.3\n",
    "#             zonei = int(row[0])\n",
    "#             msoa = row[1]\n",
    "#             name = row[2]\n",
    "#             east = float(row[3])\n",
    "#             north = float(row[4])\n",
    "#             ZoneLookup[msoa] = {\n",
    "#                 'zonei': zonei,\n",
    "#                 'name': name,\n",
    "#                 'east': east,\n",
    "#                 'north': north\n",
    "#             }\n",
    "#             #print(\"loadZoneLookup: \",msoa,zonei,name,east,north) #DEBUG\n",
    "#     return ZoneLookup\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Build a trips matrix from data in the Census CSV file (i.e. wu03ew_v2.csv).\n",
    "The point behind this is to be able to build separate matrices from the sums of a user defined set of columns.\n",
    "This does assume that the two area keys for the origin destination areas are the first two columns in the file.\n",
    "NOTE: this defines which way round the i and j are. See comment in code main loop below.\n",
    "@param name=\"CSVFilename\" string\n",
    "@param name=\"ZoneLookup\" Lookup between MSOA area key and ZoneCode index number Dictionary of dictionaries\n",
    "@param name=\"ColumnNames\" Names of columns added together to get the total trips i.e. \"UNDERGROUND\", \"TRAIN\", \"BUS\". These must be in the CSV header line List of string\n",
    "@returns An observed trips matrix (numpy) matrix\n",
    "NOTE: you will get a LOT of warnings from this - as long as they're for ODxxx or S92xxx zones, then this is normal\n",
    "\"\"\"\n",
    "def generateTripsMatrix(CSVFilename, ZoneLookup, ColumnNames):\n",
    "    N = len(ZoneLookup)\n",
    "    TijObs = np.zeros(N*N).reshape(N, N)\n",
    "    for i in range(0,N): # this is basically a hack to absolutely ensure everything is zeroed - above arange sets it to col and row values?\n",
    "        for j in range(0,N):\n",
    "            TijObs[i,j]=0.0\n",
    "    print(ColumnNames,len(ColumnNames))\n",
    "\n",
    "    with open(CSVFilename) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        fields = next(reader,None)\n",
    "        # work out column index numbers for all the column names\n",
    "        ColI = [-1 for i in range(0,len(ColumnNames))] #init with -1\n",
    "        for i in range(len(fields)):\n",
    "            field = fields[i]\n",
    "            for j in range(0,len(ColumnNames)):\n",
    "                if field == ColumnNames[j]:\n",
    "                    ColI[j] = i\n",
    "        print(ColumnNames,ColI)\n",
    "\n",
    "        # \"Area of residence\",\"Area of workplace\",\"All categories: Method of travel to work\",\"Work mainly at or from home\",\"Underground, metro, light rail, tram\",\"Train\",\"Bus, minibus or coach\",\"Taxi\",\"Motorcycle, scooter or moped\",\"Driving a car or van\",\"Passenger in a car or van\",\"Bicycle\",\"On foot\",\"Other method of travel to work\"\n",
    "        # E02000001,E02000001,1506,0,73,41,32,9,1,8,1,33,1304,4\n",
    "        lineCount = 1\n",
    "        for row in reader:\n",
    "            lineCount+=1\n",
    "            ZoneR = row[0]\n",
    "            ZoneW = row[1]\n",
    "            sum = 0\n",
    "            for i in range(len(ColI)):\n",
    "                count = int(row[ColI[i]])\n",
    "                sum += count\n",
    "            RowR = ZoneLookup.get(ZoneR,'Empty') # could potentially fail if ZoneR or ZoneW didn't exist in the shapefile\n",
    "            RowW = ZoneLookup.get(ZoneW,'Empty')\n",
    "            if RowR == 'Empty' or RowW == 'Empty':\n",
    "                print(\"Warning: trip \" + ZoneR + \" to \" + ZoneW + \" zones not found - skipped\")\n",
    "                continue\n",
    "            ZoneR_idx = RowR[\"zonei\"]\n",
    "            ZoneW_idx = RowW[\"zonei\"]\n",
    "            # TijObs[ZoneR_idx, ZoneW_idx] = sum #this was the original that was apparently the wrong way around\n",
    "            TijObs[ZoneW_idx, ZoneR_idx] = sum #this is the above line with i and j flipped - right way around\n",
    "        print('Loaded ', CSVFilename, ' and processed ', lineCount, ' lines of data')\n",
    "        print('Finished GenerateTripsMatrix')\n",
    "\n",
    "    return TijObs\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Load a numpy matrix from a file\n",
    "\"\"\"\n",
    "def loadMatrix(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        matrix = pickle.load(f)\n",
    "    return matrix\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Save a numpy matrix to a file\n",
    "\"\"\"\n",
    "def saveMatrix(matrix,filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(matrix,f)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Load a QUANT format matrix into python.\n",
    "A QUANT matrix stores the row count (m), column count (n) and then m x n IEEE754 floats (4 byte) of data\n",
    "\"\"\"\n",
    "def loadQUANTMatrix(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        (m,) = struct.unpack('i', f.read(4))\n",
    "        (n,) = struct.unpack('i', f.read(4))\n",
    "        print(\"loadQUANTMatrix::m=\",m,\"n=\",n)\n",
    "        matrix = np.arange(m*n,dtype=np.float).reshape(m, n) #and hopefully m===n, but I'm not relying on it\n",
    "        for i in range(0,m):\n",
    "            data = struct.unpack('{0}f'.format(n), f.read(4*n)) #read a row back from the stream - data=list of floats\n",
    "            for j in range(0,n):\n",
    "                matrix[i,j] = data[j]\n",
    "    return matrix\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Load CSV data from QUANT in the form of i,j,Oi,Dj,Cij,Tij\n",
    "The N parameter specifies the matrix size i.e. 7201. Technically we could read\n",
    "this from the csv if it was complete, but it's easier to pass in\n",
    "NOTE: the csv data is already in the right format to make training data from, but\n",
    "this puts it back into a Tij and Cij matrix that we use to build it from again\n",
    "\"\"\"\n",
    "def loadQUANTCSV(filename,N):\n",
    "    Tij = np.zeros(N*N).reshape(N,N)\n",
    "    Cij = np.zeros(N*N).reshape(N,N)\n",
    "    with open(filename,'r') as f:\n",
    "        lines=f.readlines() # read everything in to memory at once - it's a lot faster, but there are 52M\n",
    "        for n in range(1,len(lines)): # need to skip the header\n",
    "            # so it's i,j,Oi,Dj,Cij,Tij\n",
    "            fields = lines[n].split(',')\n",
    "            if len(fields)==6:\n",
    "                i = int(fields[0])\n",
    "                j = int(fields[1])\n",
    "                # Oi = float(fields[2])\n",
    "                # Dj = float(fields[3])\n",
    "                valCij = float(fields[4])\n",
    "                valTij = float(fields[5]) # or int?\n",
    "                Tij[i,j]=valTij\n",
    "                Cij[i,j]=valCij\n",
    "    return Cij, Tij\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Resize a matrix: if smaller then rows and cols are cut out, if bigger then rows and cols are repeated modulo\n",
    "This is used for the benchmarks, so I can vary the matrix size and see how long it takes\n",
    "\"\"\"\n",
    "def resizeMatrix(matrix,N):\n",
    "    (m, n) = np.shape(matrix) # original matrix size\n",
    "    m2 = np.zeros(N*N).reshape(N, N)\n",
    "    for i in range(0,N):\n",
    "        for j in range(0,N):\n",
    "            m2[i,j]=matrix[i % n,j % n]\n",
    "    return m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QUANTJobsModel(QUANTLHModel):\n",
    "    \"\"\"\n",
    "    constructor\n",
    "    @param n number of residential zones\n",
    "    @param m number of retail zones\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, n):\n",
    "        # constructor\n",
    "        super().__init__(m, n)\n",
    "\n",
    "    \"\"\"\n",
    "    loadEmploymentData\n",
    "    - Load jobs by zone: df with columns = [,zonei,employment]\n",
    "    - Load floorspace by zone: df with columns = [,zonei,floorspace] \n",
    "    return dfzones, dfattractors (they are DataFrames)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def loadEmploymentData_HHAttractiveness(filename_pop, filename_attractiveness, Scenario):\n",
    "        missing_values = ['-', 'n/a', 'na', '--', ' -   ']\n",
    "\n",
    "        # load population data for MSOA residential zones\n",
    "        df = pd.read_csv(filename_pop,\n",
    "                         usecols=['geography code', 'Age: All categories: Age; measures: Value'],\n",
    "                         na_values=missing_values)  # import population data for England and Wales\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)  # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "\n",
    "        # Rename columns:\n",
    "        df.rename(columns={'geography code': 'zonei',\n",
    "                           'Age: All categories: Age; measures: Value': 'Population_tot'}, inplace=True)\n",
    "\n",
    "        if Scenario == '2011':\n",
    "            df2 = pd.read_csv(filename_attractiveness, encoding='latin-1') # encoding added to solve: UnicodeDecodeError\n",
    "        elif Scenario == 'NewHousingDev_2019':\n",
    "            df2 = pd.read_csv(filename_attractiveness, encoding='latin-1') # encoding added to solve: UnicodeDecodeError\n",
    "        elif Scenario == 'NewHousingDev_2030':\n",
    "            df2 = pd.read_csv(filename_attractiveness, encoding='latin-1') # encoding added to solve: UnicodeDecodeError\n",
    "\n",
    "        df2.rename(columns={'MSOA_Code': 'zonei', 'N_of_Dwellings': 'N_of_Dwellings'}, inplace=True)\n",
    "\n",
    "        # Convert N_of_Dwellings to float\n",
    "        df2['N_of_Dwellings'] = df2.N_of_Dwellings.astype(float)\n",
    "\n",
    "        dfzones = pd.DataFrame({'zonei': df.zonei, 'Population_tot': df.Population_tot})\n",
    "        dfattractors = pd.DataFrame({'zonei': df2.zonei, 'N_of_Dwellings': df2.N_of_Dwellings})\n",
    "        return dfzones, dfattractors\n",
    "\n",
    "    @staticmethod\n",
    "    def loadEmploymentData_JobAttractiveness(filename_jobs, filename_attractiveness): # Attractiveness: offices floorspace\n",
    "        missing_values = ['-', 'n/a', 'na', '--', ' -   ']\n",
    "\n",
    "        df = pd.read_csv(filename_jobs,\n",
    "                         usecols=['geography code','Industry: All categories: Industry; measures: Value'], # select the columms that I need from file\n",
    "                         na_values=missing_values)\n",
    "        #df.dropna(axis=0, inplace=True) # I think I want to keep all the MSOAs even if they have null alues - in case change them\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)  # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "\n",
    "        # Rename columns:\n",
    "        df.rename(columns={'geography code': 'zonei', 'Industry: All categories: Industry; measures: Value': 'employment_tot'}, inplace=True)\n",
    "\n",
    "        df2 = pd.read_csv(filename_attractiveness,\n",
    "                          encoding='latin-1', # added to solve: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf4 in position 25: invalid continuation byte\n",
    "                         usecols=['Geography', 'area_code', 'Area_Name', 'Floorspace_2018-19_Total'])  # select the columms that I need from file\n",
    "                         #na_values=missing_values)\n",
    "        #df2.dropna(axis=0, inplace=True) # I think I want to keep all the MSOAs even if they have null alues - in case change them\n",
    "\n",
    "        # filter out MSOA rows in jobs_floorspace:\n",
    "        df2 = df2[df2.Geography == 'MSOA']\n",
    "\n",
    "        df2.reset_index(drop=True, inplace=True)  # IMPORTANT, otherwise indexes remain for ALL the rows i.e. idx=0..OriginalN NOT true row count!\n",
    "\n",
    "        df2.rename(columns={'area_code': 'zonei', 'Floorspace_2018-19_Total': 'floorspace'}, inplace=True)\n",
    "\n",
    "        for m in missing_values:\n",
    "            df2['floorspace'] = df2.floorspace.str.replace(m,'1.0') # Replace missing values with the minimum value of df2: 1.0\n",
    "\n",
    "        # Convert floorspace to float\n",
    "        df2['floorspace'] = df2.floorspace.str.replace(',', '').astype(float)\n",
    "\n",
    "        #df2.fillna(1.0)  # Replace nan with the minimum value of df2 - 1.0\n",
    "\n",
    "        dfzones = pd.DataFrame({'zonei': df.zonei, 'employment_tot': df.employment_tot})\n",
    "        dfattractors = pd.DataFrame({'zonei': df2.zonei, 'floorspace': df2.floorspace})\n",
    "        return dfzones, dfattractors\n",
    "\n",
    "    @staticmethod\n",
    "    def loadObsData():\n",
    "        # load observed trips for each mode:\n",
    "        SObs_0 = loadMatrix(os.path.join('data', 'SObs_1_OXF.bin'))\n",
    "        SObs_1 = loadMatrix(os.path.join('data', 'SObs_2_OXF.bin'))\n",
    "        SObs_2 = loadMatrix(os.path.join('data', 'SObs_3_OXF.bin'))\n",
    "\n",
    "        return SObs_0, SObs_1, SObs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "HHZones, HHAttractors = QUANTJobsModel.loadEmploymentData_HHAttractiveness('data/QS103EW_MSOA_OXF.csv', 'data/OXF-Dwellings2011.csv', '2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HHZones.to_csv('data/jobs_Pop_Zones_2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HHAttractors.to_csv('data/jobs_HH_Attractors_2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SObs_road, SObs_bus, SObs_rail = QUANTJobsModel.loadObsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cij as cost matrix (MSOA to MSOA)\n",
    "m, n = cij_road_OXF.shape\n",
    "model = QUANTJobsModel(m, n)\n",
    "model.setObsMatrix(SObs_road, SObs_bus, SObs_rail)\n",
    "model.setAttractorsAj(HHAttractors, 'zonei', 'N_of_Dwellings')\n",
    "model.setPopulationEi(jobs, 'zonei', 'employment_tot')\n",
    "model.setCostMatrixCij(cij_road_OXF, cij_bus_OXF, cij_rail_OXF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3modes(self):\n",
    "        # run model\n",
    "        # i = employment zone\n",
    "        # j = residential zone\n",
    "        # Ei = number of jobs at MSOA location\n",
    "        # Aj = attractor of HH (number of dwellings)\n",
    "        # cij_mode = travel cost for \"mode\" (i.e. road, bus, rail)\n",
    "        # Modes: Road = 0, Bus = 1, Rail = 2\n",
    "        # Beta = Beta values for three modes - this is also output\n",
    "        # QUANT data:\n",
    "        # Observed trips data: \"SObs_1.bin\", \"SObs_2.bin\", \"SObs_3.bin\"\n",
    "        # Travel cost per mode: \"dis_roads_min.bin\", \"dis_bus_min.bin\", \"dis_gbrail_min.bin\"\n",
    "        # Note the use of 1,2,3 for modes in the files different from 0,1,2 in the code.\n",
    "        # Returns predicted flows per mode: \"SPred_1.bin\", \"SPred_2.bin\", \"SPred_3.bin\"\n",
    "\n",
    "        # Initialisation of parameters\n",
    "        converged = False  # initialise convergence check param\n",
    "        n_modes = 3  # Number of modes\n",
    "        cij_k = [self.cij_0, self.cij_1, self.cij_2]  # list of cost matrices\n",
    "        SObs_k = [self.SObs_0, self.SObs_1, self.SObs_2]  # list of obs trips matrices\n",
    "\n",
    "        # Set up Beta for modes 0, 1 and 2 to 1.0\n",
    "        Beta = np.ones(n_modes)\n",
    "\n",
    "        # Compute sum of origins and destinations\n",
    "\n",
    "        '''\n",
    "        # OiObs : vector with dimension = number of oringins\n",
    "        OiObs = np.zeros(self.n)\n",
    "        for k in range(n_modes):\n",
    "            OiObs += SObs_k[k].sum(axis=1)\n",
    "        '''\n",
    "\n",
    "        # DjObs : vector with dimension = number of destinations\n",
    "        DjObs = [[] for i in range(n_modes)]\n",
    "        for k in range(n_modes):\n",
    "            DjObs[k] = np.zeros(self.n)\n",
    "        for k in range(n_modes):\n",
    "            DjObs[k] += SObs_k[k].sum(axis=1)\n",
    "\n",
    "        DjPred = [[] for i in range(n_modes)]\n",
    "        DjPredSum = np.zeros(n_modes)\n",
    "        DjObsSum = np.zeros(n_modes)\n",
    "        delta = np.zeros(n_modes)\n",
    "\n",
    "\n",
    "        # Convergence loop:\n",
    "        print(\"Calibrating the model...\")\n",
    "        iteration = 0\n",
    "        while converged != True:\n",
    "            iteration += 1\n",
    "            print(\"Iteration: \", iteration)\n",
    "\n",
    "            # Initialise variables:\n",
    "            Sij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty lists equal to n_modes\n",
    "\n",
    "            # hold copy of pre multiplied copies of -Beta_k * cij[k] for each mode\n",
    "            ExpMBetaCijk = [[] for k in range(n_modes)]\n",
    "            for kk in range(n_modes):\n",
    "                ExpMBetaCijk[kk] = np.exp(-Beta[kk] * cij_k[kk])\n",
    "\n",
    "            for k in range(n_modes):  # mode loop\n",
    "                Sij[k] = np.zeros(self.m * self.n).reshape(self.m, self.n)\n",
    "                for i in range(self.m):\n",
    "                    denom = 0\n",
    "                    for kk in range(n_modes):\n",
    "                        denom += np.sum(self.Aj * ExpMBetaCijk[kk][i, :])\n",
    "                    Sij2 = self.Ei[i] * (self.Aj * ExpMBetaCijk[k][i, :] / denom)\n",
    "                    Sij[k][i, :] = Sij2  # put answer slice back in return array\n",
    "\n",
    "            # Calibration with CBar values\n",
    "            # Calculate mean predicted trips and mean observed trips (this is CBar)\n",
    "            CBarPred = np.zeros(n_modes)\n",
    "            CBarObs = np.zeros(n_modes)\n",
    "            delta = np.ones(n_modes)\n",
    "            for k in range(n_modes):\n",
    "                CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "                CBarObs[k] = self.computeCBar(SObs_k[k], cij_k[k])\n",
    "                delta[k] = np.absolute(CBarPred[k] - CBarObs[k])  # the aim is to minimise delta[0]+delta[1]+...\n",
    "            # delta check on all betas (Beta0, Beta1, Beta2) stopping condition for convergence\n",
    "            # double gradient descent search on Beta0 and Beta1 and Beta2\n",
    "            converged = True\n",
    "            for k in range(n_modes):\n",
    "                if (delta[k] / CBarObs[k] > 0.001):\n",
    "                    Beta[k] = Beta[k] * CBarPred[k] / CBarObs[k]\n",
    "                    converged = False\n",
    "            '''\n",
    "            # Calibration with Observed flows\n",
    "            for k in range(n_modes):\n",
    "                DjPred[k] = self.calculateDj(Sij[k])\n",
    "                DjPredSum[k] = np.sum(DjPred[k])\n",
    "                DjObsSum[k] = np.sum(DjObs[k])\n",
    "                delta[k] = DjPredSum[k] - DjObsSum[k]\n",
    "            # delta check on beta stopping condition for convergence\n",
    "            # gradient descent search on beta\n",
    "            converged = True\n",
    "            for k in range(n_modes):\n",
    "                if (delta[k] / DjObsSum[k] > 0.001):\n",
    "                    Beta[k] = Beta[k] * DjPredSum[k] / DjObsSum[k]\n",
    "                    converged = False\n",
    "            '''\n",
    "            CBarPred = np.zeros(n_modes)\n",
    "            # Calculate CBar\n",
    "            for k in range(n_modes):\n",
    "                CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "\n",
    "            # Debug:\n",
    "            # commuter sum blocks\n",
    "            TotalSij_roads = Sij[0].sum()\n",
    "            TotalSij_bus = Sij[1].sum()\n",
    "            TotalSij_rail = Sij[2].sum()\n",
    "            TotalEi = self.Ei.sum()  # total jobs = pu+pr above\n",
    "            # print(\"i= {0:d} beta_roads={1:.6f} beta_bus={2:.6f} beta_rail={3:.6f} cbar_pred_roads={4:.1f} cbar_pred_busr={5:.1f} cbar_pred_rail={6:.1f}\"\n",
    "            #         .format(iteration, Beta[0], Beta[1], Beta[2], CBarPred[0], CBarPred[1], CBarPred[2]))\n",
    "            # print(\"TotalSij_roads={0:.1f} TotalSij_bus={1:.1f} TotalSij_rail={2:.1f} Total={3:.1f} ({4:.1f})\"\n",
    "            #       .format(TotalSij_roads, TotalSij_bus, TotalSij_rail, TotalSij_roads + TotalSij_bus + TotalSij_rail, TotalEi))\n",
    "\n",
    "        return Sij, Beta, CBarPred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating the model...\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n"
     ]
    }
   ],
   "source": [
    "Tij, beta_k, cbar_k = model.run3modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3127422 , 0.07877828, 0.05523665])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3127422 , 0.07877828, 0.05523665])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3modes_NoCalibration(self, Beta):\n",
    "        n_modes = len(Beta)  # Number of modes\n",
    "        print(\"Running model for \", n_modes, \" modes.\")\n",
    "\n",
    "        # Initialise variables:\n",
    "        Sij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty lists equal to n_modes\n",
    "        cij_k = [self.cij_0, self.cij_1, self.cij_2]  # list of cost matrices\n",
    "\n",
    "        # hold copy of pre multiplied copies of -Beta_k * cij[k] for each mode\n",
    "        ExpMBetaCijk = [[] for k in range(n_modes)]\n",
    "        for kk in range(n_modes):\n",
    "            ExpMBetaCijk[kk] = np.exp(-Beta[kk] * cij_k[kk])\n",
    "\n",
    "        for k in range(n_modes):  # mode loop\n",
    "            Sij[k] = np.zeros(self.m * self.n).reshape(self.m, self.n)\n",
    "            for i in range(self.m):\n",
    "                denom = 0\n",
    "                for kk in range(n_modes):\n",
    "                    denom += np.sum(self.Aj * ExpMBetaCijk[kk][i, :])\n",
    "                Sij2 = self.Ei[i] * (self.Aj * ExpMBetaCijk[k][i, :] / denom)\n",
    "                Sij[k][i, :] = Sij2  # put answer slice back in return array\n",
    "\n",
    "        CBarPred = np.zeros(n_modes)  # initialise CBarPred\n",
    "        for k in range(n_modes):\n",
    "            CBarPred[k] = self.computeCBar(Sij[k], cij_k[k])\n",
    "\n",
    "        return Sij, CBarPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model for  3  modes.\n"
     ]
    }
   ],
   "source": [
    "Rij, cbar = model.run3modes_NoCalibration(beta_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeProbabilities3modes(self, Sij):\n",
    "        print(\"Computing probabilities\")\n",
    "        n_modes = 3\n",
    "        probSij = [[] for i in range(n_modes)]  # initialise Sij with a number of empty list equal to n_modes\n",
    "\n",
    "        for k in range(n_modes):\n",
    "            probSij[k] = np.arange(self.m * self.n, dtype=np.float).reshape(self.m, self.n)\n",
    "            for i in range(self.m):\n",
    "                sum = np.sum(Sij[k][i,])\n",
    "                if sum <= 0:\n",
    "                    sum = 1  # catch for divide by zero - just let the zero probs come through to the final matrix\n",
    "                probSij[k][i,] = Sij[k][i,] / sum\n",
    "\n",
    "        return probSij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing probabilities\n"
     ]
    }
   ],
   "source": [
    "# Compute Probabilities:\n",
    "popretail_probRij = model.computeProbabilities3modes(Rij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/populationretailProbRij_roads_2011.csv', popretail_probRij[0], delimiter=\",\")\n",
    "np.savetxt('data/populationretailProbRij_bus_2011.csv', popretail_probRij[1], delimiter=\",\")\n",
    "np.savetxt('data/populationretailProbRij_rail_2011.csv', popretail_probRij[2], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/populationretailRij_roads_2011.csv', Rij[0], delimiter=\",\")\n",
    "np.savetxt('data/populationretailRij_bus_2011.csv', Rij[1], delimiter=\",\")\n",
    "np.savetxt('data/populationretailRij_rail_2011.csv', Rij[2], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file('data/ESRI/MSOA_2011_London_gen_MHW.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs('EPSG: 4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/l2ny8d3j1rl_k0h8ghjdkflm0000gn/T/ipykernel_3470/1845079915.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['geometry'] = gdf['geometry'].centroid\n"
     ]
    }
   ],
   "source": [
    "gdf['geometry'] = gdf['geometry'].centroid\n",
    "\n",
    "gdf = gdf[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file('data/OXF_MSOA_centroids_WGS84.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import networkx as nx\n",
    "import os\n",
    "import osmnx as ox\n",
    "\n",
    "def calc_shortest_paths_ODs_osm(zones_centroids, network):\n",
    "    # For each zone centroid, this function calculates the closest node in the OSM graph.\n",
    "    # These nodes will be used as origins and destinations in the shortest paths calculations.\n",
    "    list_of_ODs = []\n",
    "    for c in zones_centroids:\n",
    "        graph_clostest_node = ox.nearest_nodes(network, c[0], c[1], return_dist=False)\n",
    "        list_of_ODs.append(graph_clostest_node)\n",
    "    return list_of_ODs\n",
    "\n",
    "def flows_map_creation(inputs, outputs, flows_output_keys): # Using OSM\n",
    "\n",
    "    Zone_nodes = nx.read_shp('data/OXF_MSOA_centroids_WGS84.shp') # Must be in epsg:4326 (WGS84)\n",
    "\n",
    "\n",
    "    Case_Study_Zones = ['London']\n",
    "    # Case_Study_Zones = ['Oxford'] # Smaller network for test purposes\n",
    "\n",
    "    X = ox.graph_from_place(Case_Study_Zones, network_type='drive')\n",
    "    # crs = X.graph[\"crs\"]\n",
    "    # print('Graph CRS: ', crs)\n",
    "    # print()\n",
    "\n",
    "    # ox.plot_graph(X) # test plot\n",
    "\n",
    "    X = X.to_undirected()\n",
    "\n",
    "    # Calculate the origins and destinations for the shortest paths algorithms to be run on OSM graph\n",
    "    OD_list = calc_shortest_paths_ODs_osm(Zone_nodes, X)\n",
    "\n",
    "    Flows = []\n",
    "\n",
    "    for kk, flows_output_key in enumerate(flows_output_keys):\n",
    "        Flows.append(pd.read_csv(outputs[flows_output_key], header=None))\n",
    "\n",
    "        # Initialise weights to 0:\n",
    "        for source, target in X.edges():\n",
    "            X[source][target][0][\"Flows_\" + str(kk)] = 0\n",
    "\n",
    "    TOT_count = len(OD_list)\n",
    "    # print(OD_list)\n",
    "\n",
    "    for n, i in enumerate(OD_list):\n",
    "        print(\"Flows maps creation - iteration \", n+1, \" of \", TOT_count)\n",
    "        sssp_paths = nx.single_source_dijkstra_path(X, i, weight='length') # single source shortest paths from i to all nodes of the network\n",
    "        for m, j in enumerate(OD_list):\n",
    "            shortest_path = sssp_paths[j] # shortest path from i to j\n",
    "            path_edges = zip(shortest_path, shortest_path[1:])  # Create edges from nodes of the shortest path\n",
    "\n",
    "            for edge in list(path_edges):\n",
    "                for cc in range(len(Flows)):\n",
    "                    X[edge[0]][edge[1]][0][\"Flows_\" + str(cc)] += Flows[cc].iloc[n, m]\n",
    "\n",
    "    # save graph to shapefile\n",
    "    output_folder_path = \"data\" + \"Flows_shp\"\n",
    "    ox.save_graph_shapefile(X, filepath=output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import networkx as nx\n",
    "import os\n",
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pinlyu/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def calc_shortest_paths_ODs_osm(zones_centroids, network):\n",
    "    list_of_ODs = []\n",
    "    for c in zones_centroids.geometry:\n",
    "        graph_clostest_node = ox.nearest_nodes(network, c.x, c.y, return_dist=False)\n",
    "        list_of_ODs.append(graph_clostest_node)\n",
    "    return list_of_ODs\n",
    "\n",
    "def flows_map_creation(outputs, flows_output_keys):\n",
    "    # Read shapefile using GeoPandas\n",
    "    Zone_nodes = gpd.read_file('data/OXF_MSOA_centroids_WGS84.shp')\n",
    "\n",
    "    Case_Study_Zones = ['London']\n",
    "\n",
    "    X = ox.graph_from_place(Case_Study_Zones, network_type='drive')\n",
    "    X = X.to_undirected()\n",
    "\n",
    "    # Convert GeoDataFrame to suitable format for calc_shortest_paths_ODs_osm function\n",
    "    OD_list = calc_shortest_paths_ODs_osm(Zone_nodes, X)\n",
    "\n",
    "    Flows = []\n",
    "\n",
    "    for kk, flows_output_key in enumerate(flows_output_keys):\n",
    "        Flows.append(pd.read_csv(outputs[flows_output_key], header=None))\n",
    "\n",
    "        # Initialise weights to 0:\n",
    "        for source, target in X.edges():\n",
    "            X[source][target][0][\"Flows_\" + str(kk)] = 0\n",
    "\n",
    "    TOT_count = len(OD_list)\n",
    "\n",
    "    for n, i in enumerate(OD_list):\n",
    "        print(\"Flows maps creation - iteration \", n+1, \" of \", TOT_count)\n",
    "        sssp_paths = nx.single_source_dijkstra_path(X, i, weight='length') \n",
    "        for m, j in enumerate(OD_list):\n",
    "            shortest_path = sssp_paths[j] \n",
    "            path_edges = zip(shortest_path, shortest_path[1:])  \n",
    "\n",
    "            for edge in list(path_edges):\n",
    "                for cc in range(len(Flows)):\n",
    "                    X[edge[0]][edge[1]][0][\"Flows_\" + str(cc)] += Flows[cc].iloc[n, m]\n",
    "\n",
    "    # save graph to shapefile using PyShp\n",
    "    w = shp.Writer('data/Flows_shp')\n",
    "    w.field('ID')\n",
    "    id = 0\n",
    "    for edge in X.edges(data=True):\n",
    "        w.line([[[edge[0][1], edge[0][0]], [edge[1][1], edge[1][0]]]])\n",
    "        w.record(id)\n",
    "        id += 1\n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
